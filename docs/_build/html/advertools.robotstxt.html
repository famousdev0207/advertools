

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>🤖 robots.txt Tester for Large Scale Testing &mdash;  Python</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Download, Parse, and Analyze XML Sitemaps" href="advertools.sitemaps.html" />
    <link rel="prev" title="Create Ads Using Long Descriptive Text (top-down approach)" href="advertools.ad_from_string.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> advertools
          

          
          </a>

          
            
            
              <div class="version">
                0.10.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="readme.html">About advertools</a></li>
</ul>
<p class="caption"><span class="caption-text">SEM</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="advertools.kw_generate.html">Generate SEM Keywords</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.ad_create.html">Create Text Ads on a Large Scale</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.ad_from_string.html">Create Text Ads From Description Text</a></li>
</ul>
<p class="caption"><span class="caption-text">SEO</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">robots.txt</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#user-agents">User-agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="#robots-txt-testing-approach">robots.txt Testing Approach</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="advertools.sitemaps.html">XML Sitemaps</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.spider.html">SEO Spider / Crawler</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.serp.html">Analyze Search Engine Results (SERPs)</a></li>
</ul>
<p class="caption"><span class="caption-text">Text &amp; Content Analysis</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="advertools.emoji.html">Emoji Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.extract.html">Extract Structured Entities from Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.stopwords.html">Stop Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.word_frequency.html">Text Analysis (absolute &amp; weighted word frequency)</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.word_tokenize.html">Word Tokenization (N-grams)</a></li>
</ul>
<p class="caption"><span class="caption-text">Social Media</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="advertools.twitter.html">Twitter Data API</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.youtube.html">YouTube Data API</a></li>
</ul>
<p class="caption"><span class="caption-text">Index &amp; Change Log</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="include_changelog.html">Index &amp; Change Log</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">advertools</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>🤖 robots.txt Tester for Large Scale Testing</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/advertools.robotstxt.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <span class="target" id="module-advertools.robotstxt"></span><div class="section" id="robots-txt-tester-for-large-scale-testing">
<span id="robotstxt"></span><h1>🤖 robots.txt Tester for Large Scale Testing<a class="headerlink" href="#robots-txt-tester-for-large-scale-testing" title="Permalink to this headline">¶</a></h1>
<p>Even though they are tiny in size, robots.txt files contain potent information
that can block major sections of your site, which is what they are supposed to
do. Only sometimes you might make the mistake of blocking the wrong section.</p>
<p>So it is very important to check if certain pages (or groups of pages) are
blocked for a certain user-agent by a certain robots.txt file. Ideally, you
would want to run the same check for all possible user-agents. Even more
ideally, you want to be able to run the check for a large number of pages with
every possible combination with user-agents!</p>
<p>To get the robots.txt file into an easily readable format, you can use the
<a class="reference internal" href="#advertools.robotstxt.robotstxt_to_df" title="advertools.robotstxt.robotstxt_to_df"><code class="xref py py-func docutils literal notranslate"><span class="pre">robotstxt_to_df()</span></code></a> function to get it in a DataFrame.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">robotstxt_to_df</span><span class="p">(</span><span class="s1">&#39;https://www.google.com/robots.txt&#39;</span><span class="p">)</span>
<span class="go">      directive                             content                      robotstxt_url                  file_downloaded</span>
<span class="go">0    User-agent                                   *  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="go">1      Disallow                             /search  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="go">2         Allow                       /search/about  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="go">3         Allow                      /search/static  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="go">4         Allow              /search/howsearchworks  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="go">..          ...                                 ...                                ...                              ...</span>
<span class="go">277  User-agent                          Twitterbot  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="go">278       Allow                             /imgres  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="go">279  User-agent                 facebookexternalhit  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="go">280       Allow                             /imgres  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="go">281     Sitemap  https://www.google.com/sitemap.xml  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="go">[282 rows x 4 columns]</span>
</pre></div>
</div>
<p>The returned DataFrame contains columns for directives, their content, the URL
of the robots.txt file, as well as the date it was downloaded.
Under the <cite>directive</cite> column you can see the main commands; Allow, Disallow,
Sitemap, Crawl-delay, User-agent, and so on. The <cite>content</cite> column contains the
details of each of those directives (the pattern to disallow, the sitemap URL,
etc.)</p>
<p>As for testing, the <a class="reference internal" href="#advertools.robotstxt.robotstxt_test" title="advertools.robotstxt.robotstxt_test"><code class="xref py py-func docutils literal notranslate"><span class="pre">robotstxt_test()</span></code></a> function runs a test for a given
robots.txt file, checking which of the provided user-agents can fetch which of
the provided URLs, paths, or patterns.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">robotstxt_test</span><span class="p">(</span><span class="s1">&#39;https://www.example.com/robots.txt&#39;</span><span class="p">,</span>
<span class="gp">... </span>               <span class="n">useragents</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Googlebot&#39;</span><span class="p">,</span> <span class="s1">&#39;baiduspider&#39;</span><span class="p">,</span> <span class="s1">&#39;Bingbot&#39;</span><span class="p">]</span>
<span class="gp">... </span>               <span class="n">urls</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="s1">&#39;/hello&#39;</span><span class="p">,</span> <span class="s1">&#39;/some-page.html&#39;</span><span class="p">]])</span>
</pre></div>
</div>
<p>As a result, you get a DataFrame with a row for each combination of
(user-agent, URL) indicating whether or not that particular user-agen can fetch
the given URL.</p>
<p>Some reasons why you might want to do that:</p>
<ul class="simple">
<li><p>SEO Audits: Especially for large websites with many URL patterns, and many
rules for different user-agents.</p></li>
<li><p>Developer or site owner about to make large changes</p></li>
<li><p>Interest in strategies of certain companies</p></li>
</ul>
<div class="section" id="user-agents">
<h2>User-agents<a class="headerlink" href="#user-agents" title="Permalink to this headline">¶</a></h2>
<p>In reality there are only two groups of user-agents that you need to worry
about:</p>
<ul class="simple">
<li><p>User-agents listed in the robots.txt file: For each one of those you need to
check whether or not they are blocked from a fetching a certain URL
(or pattern).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*</span></code> all other user-agents: The <code class="docutils literal notranslate"><span class="pre">*</span></code> includes all other user-agents, so
checking the rules that apply to it should take care of the rest.</p></li>
</ul>
</div>
<div class="section" id="robots-txt-testing-approach">
<h2>robots.txt Testing Approach<a class="headerlink" href="#robots-txt-testing-approach" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Get the robots.txt file that you are interested in</p></li>
<li><p>Extract the user-agents from it</p></li>
<li><p>Specify the URLs you are interested in testing</p></li>
<li><p>Run the <a class="reference internal" href="#advertools.robotstxt.robotstxt_test" title="advertools.robotstxt.robotstxt_test"><code class="xref py py-func docutils literal notranslate"><span class="pre">robotstxt_test()</span></code></a> function</p></li>
</ol>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fb_robots</span> <span class="o">=</span> <span class="n">robotstxt_to_df</span><span class="p">(</span><span class="s1">&#39;https://www.facebook.com/robots.txt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fb_robots</span>
<span class="go">      directive                                            content                        robotstxt_url                  file_downloaded</span>
<span class="go">0       comment  Notice: Collection of data on Facebook through...  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="go">1       comment  prohibited unless you have express written per...  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="go">2       comment  and may only be conducted for the limited purp...  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="go">3       comment                                        permission.  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="go">4       comment  See: http://www.facebook.com/apps/site_scrapin...  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="go">..          ...                                                ...                                  ...                              ...</span>
<span class="go">461       Allow                         /ajax/bootloader-endpoint/  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="go">462       Allow  /ajax/pagelet/generic.php/PagePostsSectionPagelet  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="go">463       Allow                                      /safetycheck/  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="go">464  User-agent                                                  *  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="go">465    Disallow                                                  /  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="go">[466 rows x 4 columns]</span>
</pre></div>
</div>
<p>Now that we have downloaded the file, we can easily extract the list of
user-agents that it contains.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fb_useragents</span> <span class="o">=</span> <span class="p">(</span><span class="n">fb_robots</span>
<span class="gp">... </span>                 <span class="p">[</span><span class="n">fb_robots</span><span class="p">[</span><span class="s1">&#39;directive&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;User-agent&#39;</span><span class="p">]</span>
<span class="gp">... </span>                 <span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">()</span>
<span class="gp">... </span>                 <span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fb_useragents</span>
<span class="go">[&#39;Applebot&#39;,</span>
<span class="go"> &#39;baiduspider&#39;,</span>
<span class="go"> &#39;Bingbot&#39;,</span>
<span class="go"> &#39;Discordbot&#39;,</span>
<span class="go"> &#39;facebookexternalhit&#39;,</span>
<span class="go"> &#39;Googlebot&#39;,</span>
<span class="go"> &#39;Googlebot-Image&#39;,</span>
<span class="go"> &#39;ia_archiver&#39;,</span>
<span class="go"> &#39;LinkedInBot&#39;,</span>
<span class="go"> &#39;msnbot&#39;,</span>
<span class="go"> &#39;Naverbot&#39;,</span>
<span class="go"> &#39;Pinterestbot&#39;,</span>
<span class="go"> &#39;seznambot&#39;,</span>
<span class="go"> &#39;Slurp&#39;,</span>
<span class="go"> &#39;teoma&#39;,</span>
<span class="go"> &#39;TelegramBot&#39;,</span>
<span class="go"> &#39;Twitterbot&#39;,</span>
<span class="go"> &#39;Yandex&#39;,</span>
<span class="go"> &#39;Yeti&#39;,</span>
<span class="go"> &#39;*&#39;]</span>
</pre></div>
</div>
<p>Quite a long list!</p>
<p>As a small and quick test, I’m interested in checking the home page, a random
profile page (/bbc), groups and hashtags pages.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">urls_to_test</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="s1">&#39;/bbc&#39;</span><span class="p">,</span> <span class="s1">&#39;/groups&#39;</span><span class="p">,</span> <span class="s1">&#39;/hashtag/&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fb_test</span> <span class="o">=</span> <span class="n">robotstxt_test</span><span class="p">(</span><span class="s1">&#39;https://www.facebook.com/robots.txt&#39;</span><span class="p">,</span>
<span class="gp">... </span>                         <span class="n">fb_useragents</span><span class="p">,</span> <span class="n">urls_to_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fb_test</span>
<span class="go">                          robotstxt_url user_agent   url_path  can_fetch</span>
<span class="go">0   https://www.facebook.com/robots.txt          *       /bbc      False</span>
<span class="go">1   https://www.facebook.com/robots.txt          *    /groups      False</span>
<span class="go">2   https://www.facebook.com/robots.txt          *          /      False</span>
<span class="go">3   https://www.facebook.com/robots.txt          *  /hashtag/      False</span>
<span class="go">4   https://www.facebook.com/robots.txt   Applebot          /       True</span>
<span class="go">..                                  ...        ...        ...        ...</span>
<span class="go">75  https://www.facebook.com/robots.txt  seznambot    /groups       True</span>
<span class="go">76  https://www.facebook.com/robots.txt      teoma          /       True</span>
<span class="go">77  https://www.facebook.com/robots.txt      teoma  /hashtag/      False</span>
<span class="go">78  https://www.facebook.com/robots.txt      teoma       /bbc       True</span>
<span class="go">79  https://www.facebook.com/robots.txt      teoma    /groups       True</span>
<span class="go">[80 rows x 4 columns]</span>
</pre></div>
</div>
<p>For twenty user-agents and four URLs each, we received a total of eighty test
results. You can immediately see that all user-agents not listed (denoted by
“*” are not allowed to fetch any of the provided URLs).</p>
<p>Let’s see who is and who is not allowed to fetch the home page.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fb_test</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;url_path== &quot;/&quot;&#39;</span><span class="p">)</span>
<span class="go">                          robotstxt_url           user_agent  url_path  can_fetch</span>
<span class="go">2   https://www.facebook.com/robots.txt                    *         /      False</span>
<span class="go">4   https://www.facebook.com/robots.txt             Applebot         /       True</span>
<span class="go">9   https://www.facebook.com/robots.txt              Bingbot         /       True</span>
<span class="go">14  https://www.facebook.com/robots.txt           Discordbot         /      False</span>
<span class="go">18  https://www.facebook.com/robots.txt            Googlebot         /       True</span>
<span class="go">21  https://www.facebook.com/robots.txt      Googlebot-Image         /       True</span>
<span class="go">26  https://www.facebook.com/robots.txt          LinkedInBot         /      False</span>
<span class="go">30  https://www.facebook.com/robots.txt             Naverbot         /       True</span>
<span class="go">35  https://www.facebook.com/robots.txt         Pinterestbot         /      False</span>
<span class="go">39  https://www.facebook.com/robots.txt                Slurp         /       True</span>
<span class="go">43  https://www.facebook.com/robots.txt          TelegramBot         /      False</span>
<span class="go">47  https://www.facebook.com/robots.txt           Twitterbot         /       True</span>
<span class="go">48  https://www.facebook.com/robots.txt               Yandex         /       True</span>
<span class="go">55  https://www.facebook.com/robots.txt                 Yeti         /       True</span>
<span class="go">57  https://www.facebook.com/robots.txt          baiduspider         /       True</span>
<span class="go">60  https://www.facebook.com/robots.txt  facebookexternalhit         /      False</span>
<span class="go">64  https://www.facebook.com/robots.txt          ia_archiver         /      False</span>
<span class="go">68  https://www.facebook.com/robots.txt               msnbot         /       True</span>
<span class="go">74  https://www.facebook.com/robots.txt            seznambot         /       True</span>
<span class="go">76  https://www.facebook.com/robots.txt                teoma         /       True</span>
</pre></div>
</div>
<p>I’ll leave to you to figure out why LinkedIn and Pinterest are not allowed to
crawl the home page but Google and Apple are, because I have no clue!</p>
<dl class="py function">
<dt id="advertools.robotstxt.robotstxt_to_df">
<code class="sig-name descname">robotstxt_to_df</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">robotstxt_url</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/advertools/robotstxt.html#robotstxt_to_df"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#advertools.robotstxt.robotstxt_to_df" title="Permalink to this definition">¶</a></dt>
<dd><p>Download the contents of <code class="docutils literal notranslate"><span class="pre">robotstxt_url</span></code> into a DataFrame</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>robotstxt_url</strong> (<em>url</em>) – The URL of the robots.txt file</p>
</dd>
<dt class="field-even">Returns DataFrame robotstxt_df</dt>
<dd class="field-even"><p>A DataFrame containing directives, their
content, the URL and time of download</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="advertools.robotstxt.robotstxt_test">
<code class="sig-name descname">robotstxt_test</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">robotstxt_url</span></em>, <em class="sig-param"><span class="n">user_agents</span></em>, <em class="sig-param"><span class="n">urls</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/advertools/robotstxt.html#robotstxt_test"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#advertools.robotstxt.robotstxt_test" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a <code class="xref py py-attr docutils literal notranslate"><span class="pre">robotstxt_url</span></code> check which of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">user_agents</span></code> is
allowed to fetch which of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">urls</span></code>.</p>
<p>All the combinations of <code class="xref py py-attr docutils literal notranslate"><span class="pre">user_agents</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">urls</span></code> will be
checked and the results returned in one DataFrame.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">robotstxt_test</span><span class="p">(</span><span class="s1">&#39;https://facebook.com/robots.txt&#39;</span><span class="p">,</span>
<span class="gp">... </span>               <span class="n">user_agents</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="s1">&#39;Googlebot&#39;</span><span class="p">,</span> <span class="s1">&#39;Applebot&#39;</span><span class="p">],</span>
<span class="gp">... </span>               <span class="n">urls</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="s1">&#39;/bbc&#39;</span><span class="p">,</span> <span class="s1">&#39;/groups&#39;</span><span class="p">,</span> <span class="s1">&#39;/hashtag/&#39;</span><span class="p">])</span>
<span class="go">                              robotstxt_url user_agent   url_path  can_fetch</span>
<span class="go">0   https://www.facebook.com/robots.txt          *       /bbc      False</span>
<span class="go">1   https://www.facebook.com/robots.txt          *    /groups      False</span>
<span class="go">2   https://www.facebook.com/robots.txt          *          /      False</span>
<span class="go">3   https://www.facebook.com/robots.txt          *  /hashtag/      False</span>
<span class="go">4   https://www.facebook.com/robots.txt   Applebot          /       True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>robotstxt_url</strong> (<em>url</em>) – The URL of robotx.txt file</p></li>
<li><p><strong>user_agents</strong> (<em>str</em><em>,</em><em>list</em>) – One or more user agents</p></li>
<li><p><strong>urls</strong> (<em>str</em><em>,</em><em>list</em>) – One or more paths (relative) or URLs (absolute) to
check</p></li>
</ul>
</dd>
<dt class="field-even">Return DataFrame robotstxt_test_df</dt>
<dd class="field-even"><p></p></dd>
</dl>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="advertools.sitemaps.html" class="btn btn-neutral float-right" title="Download, Parse, and Analyze XML Sitemaps" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="advertools.ad_from_string.html" class="btn btn-neutral float-left" title="Create Ads Using Long Descriptive Text (top-down approach)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Elias Dabbas

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
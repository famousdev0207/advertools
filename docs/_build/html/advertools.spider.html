

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Python SEO Crawler / Spider &mdash;  Python</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Twitter Data API" href="advertools.twitter.html" />
    <link rel="prev" title="Import Search Engine Results Pages (SERPs) for Google and YouTube" href="advertools.serp.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> advertools
          

          
          </a>

          
            
            
              <div class="version">
                0.8.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="readme.html">About advertools</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.kw_generate.html">Generate SEM Keywords</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.ad_create.html">Create Text Ads on a Large Scale</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.ad_from_string.html">Create Text Ads From Description Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.emoji.html">Emoji Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.extract.html">Extract Structured Entities from Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.sitemaps.html">XML Sitemaps</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.stopwords.html">Stop Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.word_frequency.html">Text Analysis (absolute &amp; weighted word frequency)</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.word_tokenize.html">Word Tokenization (N-grams)</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.serp.html">Analyze Search Engine Results (SERPs)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">SEO Spider / Crawler</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.twitter.html">Twitter Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.youtube.html">YouTube Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="include_changelog.html">Index &amp; Change Log</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">advertools</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Python SEO Crawler / Spider</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/advertools.spider.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <span class="target" id="module-advertools.spider"></span><div class="section" id="python-seo-crawler-spider">
<h1>Python SEO Crawler / Spider<a class="headerlink" href="#python-seo-crawler-spider" title="Permalink to this headline">¶</a></h1>
<p>A straightforward crawler to analyze SEO and content of pages and websites.</p>
<p>This is provided by the <a class="reference internal" href="#advertools.spider.crawl" title="advertools.spider.crawl"><code class="xref py py-func docutils literal notranslate"><span class="pre">crawl()</span></code></a> function which is customized for SEO and
content analysis usage, and is highly configurable. The crawler uses
<a class="reference external" href="https://scrapy.org/">Scrapy</a> so you get all the power that it provides in
terms of performance, speed, as well as flexibility and customization.</p>
<p>There are two main approaches to crawl:</p>
<ol class="arabic simple">
<li><p><strong>Discovery:</strong> You know the website to crawl, so you provide a <code class="docutils literal notranslate"><span class="pre">url_list</span></code>
or the website’s sitemap(s), and you want the crawler to go through the
whole website(s) by following all available links.</p></li>
<li><p><strong>Pre-determined:</strong> You have a known set of URLs that you want to crawl and
analyze, without following links or discovering new URLs.</p></li>
</ol>
<div class="section" id="discovery">
<h2>Discovery<a class="headerlink" href="#discovery" title="Permalink to this headline">¶</a></h2>
<p>The simplest way to use the function is to provide a list of one or more
sitemap URLs. You can alternatively provide a link to a sitemap index URL, and
the crawler will go through all of the sitemaps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crawl</span><span class="p">([</span><span class="s1">&#39;https://example.com/sitemap.xml&#39;</span><span class="p">],</span> <span class="s1">&#39;my_output_file.csv&#39;</span><span class="p">)</span>  <span class="c1"># OR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">crawl</span><span class="p">([</span><span class="s1">&#39;https://example.com/sitemap-index.xml&#39;</span><span class="p">],</span> <span class="s1">&#39;my_output_file.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>That’s it!</p>
<p>What this does:</p>
<ul class="simple">
<li><p>Check the site’s robots.txt file and get the crawl rules, together with any
sitemaps if available</p></li>
<li><p>Go to the specified sitemap (or sitemap index)</p></li>
<li><p>Go through all the URLs in the sitemap(s)</p></li>
<li><p>For each URL extract the most important SEO elements</p></li>
<li><p>Save them to <code class="docutils literal notranslate"><span class="pre">output_file</span></code> in the specified format</p></li>
<li><p>The column headers of the output file (if you specify csv) would be the names
of the elements</p></li>
</ul>
<p>Supported file extensions:</p>
<ul class="simple">
<li><p>csv</p></li>
<li><p>json</p></li>
<li><p>jl</p></li>
<li><p>xml</p></li>
<li><p>marshal</p></li>
<li><p>pickle</p></li>
</ul>
</div>
<div class="section" id="extracted-on-page-seo-elements">
<h2>Extracted On-Page SEO Elements<a class="headerlink" href="#extracted-on-page-seo-elements" title="Permalink to this headline">¶</a></h2>
<p>The names of these elements become the headers (column names) of the
<code class="docutils literal notranslate"><span class="pre">output_file</span></code>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 19%" />
<col style="width: 81%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Element</p></th>
<th class="head"><p>Remarks</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>url</p></td>
<td><p>The URL requested</p></td>
</tr>
<tr class="row-odd"><td><p>title</p></td>
<td><p>The &lt;title&gt; tag(s)</p></td>
</tr>
<tr class="row-even"><td><p>meta_desc</p></td>
<td><p>Meta description</p></td>
</tr>
<tr class="row-odd"><td><p>h1</p></td>
<td><p><cite>&lt;h1&gt;</cite> tag(s)</p></td>
</tr>
<tr class="row-even"><td><p>h2</p></td>
<td><p><cite>&lt;h2&gt;</cite> tag(s)</p></td>
</tr>
<tr class="row-odd"><td><p>h3</p></td>
<td><p><cite>&lt;h3&gt;</cite> tag(s)</p></td>
</tr>
<tr class="row-even"><td><p>body_text</p></td>
<td><p>The text in the &lt;p&gt; tags</p></td>
</tr>
<tr class="row-odd"><td><p>size</p></td>
<td><p>The page size in bytes</p></td>
</tr>
<tr class="row-even"><td><p>load_time</p></td>
<td><p>The load time of the HTML (does NOT include images or videos)</p></td>
</tr>
<tr class="row-odd"><td><p>status</p></td>
<td><p>Response status (200, 301, 302, 404, etc.)</p></td>
</tr>
<tr class="row-even"><td><p>links_href</p></td>
<td><p>The links in the page <code class="docutils literal notranslate"><span class="pre">href</span></code> attribute</p></td>
</tr>
<tr class="row-odd"><td><p>links_text</p></td>
<td><p>The link titles (empty string if not available)</p></td>
</tr>
<tr class="row-even"><td><p>img_src</p></td>
<td><p>The <code class="docutils literal notranslate"><span class="pre">src</span></code> attribute of images</p></td>
</tr>
<tr class="row-odd"><td><p>img_alt</p></td>
<td><p>The <code class="docutils literal notranslate"><span class="pre">alt</span></code> attribute if available or an empty string</p></td>
</tr>
<tr class="row-even"><td><p>page_depth</p></td>
<td><p>The depth of the crawled page</p></td>
</tr>
<tr class="row-odd"><td><p>crawl_time</p></td>
<td><p>Date and time of the crawl</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All elements that may appear multiple times in a page (like header tags, or
images, for example), will be joined with two &#64; signs <cite>&#64;&#64;</cite>. For example,
<strong>“first H2 tag&#64;&#64;second H2 tag&#64;&#64;third tag”</strong> and so on.
Once you open the file, you simply have to split by <cite>&#64;&#64;</cite> to get the
elements as a list.</p>
</div>
<p>Here is a sample file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">site_crawl</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;path/to/file.csv&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">site_crawl</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="go">                                                url                                              title                                          meta_desc                                                 h1                                                 h2                                                 h3                                          body_text    size  load_time  status                                         links_href                                         links_text                                            img_src                                            img_alt  page_depth           crawl_time</span>
<span class="go">0  https://www.wsj.com/articles/u-s-extends-indiv...  U.S. Extends Individual Tax Filing Deadline fr...  The U.S. will extend the individual tax filing...       U.S. Extends Individual Tax Filing Deadl...  Tax preparers and lawmakers in both parties ur...                                                NaN  March 20, 2020 WASHINGTON—The U.S. extended t...  599133   0.828943     200  #main@@https://www.barrons.com@@http://bigchar...  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@...  https://m.wsj.net/video/20200320/032020virusne...  [https://m.wsj.net/video/20200320/032020virusn...           1  2020-03-20 23:58:13</span>
<span class="go">1  https://www.wsj.com/livecoverage/coronavirus-p...  States Hold Primary Elections in Coronavirus O...  Three states held the first primary elections,...  States Hold Primary Elections in Coronavirus O...  Three states held the first primary elections,...  Biden Wins Florida, Illinois and Arizona@@Main...  March 20, 2020 Joe Biden won all three primar...  495898   0.818214     200  #main@@https://www.barrons.com@@http://bigchar...  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@...            https://images.wsj.net/im-165947/social                                                NaN           1  2020-03-20 23:58:14</span>
<span class="go">2       https://www.wsj.com/livecoverage/coronavirus  Coronavirus Updates: Dow, Stocks Waver; Califo...  As the coronavirus pandemic roils markets and ...  Coronavirus Updates: Stocks Drop as More State...  As the coronavirus pandemic roils markets and ...  What to Know Now@@Business-Development Compani...  March 20, 2020 , which raise money from inves...  523662   0.826189     200  #main@@https://www.barrons.com@@http://bigchar...  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@...                                                NaN                                                NaN           1  2020-03-20 23:58:14</span>
<span class="go">3  https://www.wsj.com/articles/ford-marriott-ama...  Ford, Marriott, Amazon.com: Stocks That Define...  Here are seven major companies whose shares mo...       Ford, Marriott, Amazon.com: Stocks That ...  Here are seven major companies whose shares mo...                                                NaN  March 20, 2020  Ford Motor Co.  Detroit’s au...  600647   0.846521     200  #main@@https://www.barrons.com@@http://bigchar...  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@...  https://m.wsj.net/video/20200320/032020virusne...  [https://m.wsj.net/video/20200320/032020virusn...           1  2020-03-20 23:58:14</span>
<span class="go">4  https://www.wsj.com/articles/airbnb-racks-up-h...  Airbnb Racks Up Hundreds of Millions in Losses...  Airbnb is considering raising capital from new...       Airbn</span>
</pre></div>
</div>
</div>
<div class="section" id="pre-determined-crawling-approach">
<h2>Pre-Determined Crawling Approach<a class="headerlink" href="#pre-determined-crawling-approach" title="Permalink to this headline">¶</a></h2>
<p>Sometimes you might have a fixed set of URLs for which you want to scrape and
analyze SEO or content performance. Let’s say you just ran
<a class="reference internal" href="advertools.serp.html#serp"><span class="std std-ref">serp_goog</span></a> and got a bunch of top-ranking pages that you would
like to analyze, and see how that relates to their SERP ranking.</p>
<p>You simply provide the <code class="docutils literal notranslate"><span class="pre">url_list</span></code> parameter and again specify the
<code class="docutils literal notranslate"><span class="pre">output_file</span></code>. This will only crawl the specified URLs, and will not follow
any links.</p>
<p>Other than crawling SERP results landing pages, this might be useful for
monitoring certain pages that change all the time.</p>
<p>Again running the function is as simple as providing a list of URLs, as well as
a filepath where you want the result saved.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crawl</span><span class="p">(</span><span class="n">url_list</span><span class="p">,</span> <span class="n">output_file</span><span class="p">)</span>
</pre></div>
</div>
<p>The difference between the two approaches, is the simple parameter
<code class="docutils literal notranslate"><span class="pre">follow_links</span></code>. If you specify this as <code class="docutils literal notranslate"><span class="pre">False</span></code> (the default), the crawler
will only go through the provided URLs (or sitemaps). Otherwise, it will
discover pages by following links on pages that it crawls.
So how do you make sure that the crawler doesn’t try to crawl the whole web
when <code class="docutils literal notranslate"><span class="pre">follow_links</span></code> is <cite>True</cite>?
The <code class="docutils literal notranslate"><span class="pre">allowed_domains</span></code> parameter ensures that you specify which domains you
want to limit your crawler to.
This is an optional parameter. If you don’t specify it, then it will
default to only the domains in the <code class="docutils literal notranslate"><span class="pre">url_list</span></code> and/or the <code class="docutils literal notranslate"><span class="pre">sitemap_urls</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crawl</span><span class="p">(</span><span class="n">sitemap_urls</span><span class="p">,</span> <span class="n">url_list</span><span class="p">,</span> <span class="n">output_file</span><span class="p">,</span> <span class="n">allowed_domains</span><span class="p">,</span>
<span class="gp">... </span>      <span class="n">follow_links</span><span class="p">,</span> <span class="n">custom_settings</span><span class="p">)</span>
</pre></div>
</div>
<dl class="function">
<dt id="advertools.spider.crawl">
<code class="sig-name descname">crawl</code><span class="sig-paren">(</span><em class="sig-param">sitemap_urls=None</em>, <em class="sig-param">url_list=None</em>, <em class="sig-param">allowed_domains=None</em>, <em class="sig-param">follow_links=False</em>, <em class="sig-param">output_file=None</em>, <em class="sig-param">custom_settings=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/advertools/spider.html#crawl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#advertools.spider.crawl" title="Permalink to this definition">¶</a></dt>
<dd><p>Crawl a website’s URLs based on the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">sitemap_urls</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sitemap_urls</strong> (<em>list</em>) – A list of one or more XML sitemap (or sitemap
index) URLs.</p></li>
<li><p><strong>url_list</strong> (<em>list</em>) – A list of URLs to crawl. If <code class="docutils literal notranslate"><span class="pre">follow_links</span></code> is True,
the crawler will start with these URLs and follow all
links on pages recursively.</p></li>
<li><p><strong>allowed_domains</strong> (<em>list</em>) – (optional) A list of the allowed domains to
crawl. This ensures that the crawler does not
attempt to crawl the whole web. If not
specified, it defaults to the domains of the
URLs provided in <code class="docutils literal notranslate"><span class="pre">sitemap_urls</span></code> and/or
<code class="docutils literal notranslate"><span class="pre">url_list</span></code>.</p></li>
<li><p><strong>follow_links</strong> (<em>bool</em>) – Defaults to False. Whether or not to follow links
on crawled pages.</p></li>
<li><p><strong>output_file</strong> (<em>str</em>) – The path to the output of the crawl. Supported
formats ‘csv’, ‘json’, ‘jsonlines’, ‘jl’, ‘xml’,
‘marshal’, ‘pickle’.</p></li>
<li><p><strong>custom_settings</strong> (<em>dict</em>) – An optional dictionary of settings to
override.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="advertools.twitter.html" class="btn btn-neutral float-right" title="Twitter Data API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="advertools.serp.html" class="btn btn-neutral float-left" title="Import Search Engine Results Pages (SERPs) for Google and YouTube" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Elias Dabbas

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
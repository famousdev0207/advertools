

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>advertools.robotstxt &mdash;  Python</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> advertools
          

          
          </a>

          
            
            
              <div class="version">
                0.10.7
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../readme.html">About advertools</a></li>
</ul>
<p class="caption"><span class="caption-text">SEM</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.kw_generate.html">Generate SEM Keywords</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.ad_create.html">Create Text Ads on a Large Scale</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.ad_from_string.html">Create Text Ads From Description Text</a></li>
</ul>
<p class="caption"><span class="caption-text">SEO</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.robotstxt.html">robots.txt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.sitemaps.html">XML Sitemaps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.spider.html">SEO Spider / Crawler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.code_recipes.spider_strategies.html">Crawl Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.logs.html">Crawl Logs Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.serp.html">Analyze Search Engine Results (SERPs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.knowledge_graph.html">Google's Knowledge Graph</a></li>
</ul>
<p class="caption"><span class="caption-text">Text &amp; Content Analysis</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.urlytics.html">URL Structure Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.emoji.html">Emoji Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.extract.html">Extract Structured Entities from Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.stopwords.html">Stop Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.word_frequency.html">Text Analysis (absolute &amp; weighted word frequency)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.word_tokenize.html">Word Tokenization (N-grams)</a></li>
</ul>
<p class="caption"><span class="caption-text">Social Media</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.twitter.html">Twitter Data API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.youtube.html">YouTube Data API</a></li>
</ul>
<p class="caption"><span class="caption-text">Index &amp; Change Log</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../include_changelog.html">Index &amp; Change Log</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">advertools</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>advertools.robotstxt</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for advertools.robotstxt</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">.. _robotstxt:</span>

<span class="sd">ðŸ¤– robots.txt Tester for Large Scale Testing</span>
<span class="sd">============================================</span>

<span class="sd">Even though they are tiny in size, robots.txt files contain potent information</span>
<span class="sd">that can block major sections of your site, which is what they are supposed to</span>
<span class="sd">do. Only sometimes you might make the mistake of blocking the wrong section.</span>

<span class="sd">So it is very important to check if certain pages (or groups of pages) are</span>
<span class="sd">blocked for a certain user-agent by a certain robots.txt file. Ideally, you</span>
<span class="sd">would want to run the same check for all possible user-agents. Even more</span>
<span class="sd">ideally, you want to be able to run the check for a large number of pages with</span>
<span class="sd">every possible combination with user-agents!</span>

<span class="sd">To get the robots.txt file into an easily readable format, you can use the</span>
<span class="sd">:func:`robotstxt_to_df` function to get it in a DataFrame.</span>

<span class="sd">&gt;&gt;&gt; robotstxt_to_df(&#39;https://www.amazon.com/robots.txt&#39;)</span>
<span class="sd">       directive                                            content	    robotstxt_last_modified	                              etag	                      robotstxt_url	                     download_date</span>
<span class="sd">0	  User-agent	                                              *	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">1	    Disallow	              /exec/obidos/account-access-login	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">2	    Disallow	                      /exec/obidos/change-style	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">3	    Disallow	                      /exec/obidos/flex-sign-in	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">3	    Disallow	                      /exec/obidos/flex-sign-in	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">3	    Disallow	                      /exec/obidos/flex-sign-in	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">3	    Disallow	                      /exec/obidos/flex-sign-in	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">3	    Disallow	                      /exec/obidos/flex-sign-in	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">3	    Disallow	                      /exec/obidos/flex-sign-in	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">3	    Disallow	                      /exec/obidos/flex-sign-in	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">4	    Disallow	                    /exec/obidos/handle-buy-box	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">138	    Disallow	                 /gp/help/customer/express/c2c/	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">139	    Disallow	                                      /slp/*/b$	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">140	    Disallow	  /hz/contact-us/ajax/initiate-trusted-contact/	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">141	  User-agent	                                     EtaoSpider	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>
<span class="sd">142	    Disallow	                                              /	  2020-10-09 22:39:49+00:00	  8e5277c97035c645b89ceb97cdb8c619	  https://www.amazon.com/robots.txt	  2021-01-16 15:41:10.803866+00:00</span>

<span class="sd">The returned DataFrame contains columns for directives, their content, the URL</span>
<span class="sd">of the robots.txt file, as well as the date it was downloaded.</span>

<span class="sd">*  `directive`: The main commands. Allow, Disallow, Sitemap, Crawl-delay,</span>
<span class="sd">   User-agent, and so on.</span>
<span class="sd">*  `content`: The details of each of the directives</span>
<span class="sd">*  `robotstxt_last_modified`: The date when the robots.txt file was last modified</span>
<span class="sd">   (if availabe)</span>
<span class="sd">*  `etag`: The entity tag of the response header, if provided.</span>
<span class="sd">*  `robotstxt_url`: The URL of the robots.txt file.</span>
<span class="sd">*  `download_date`: The date and time when the file was downloaded.</span>


<span class="sd">As for testing, the :func:`robotstxt_test` function runs a test for a given</span>
<span class="sd">robots.txt file, checking which of the provided user-agents can fetch which of</span>
<span class="sd">the provided URLs, paths, or patterns.</span>

<span class="sd">&gt;&gt;&gt; robotstxt_test(&#39;https://www.example.com/robots.txt&#39;,</span>
<span class="sd">...                useragents=[&#39;Googlebot&#39;, &#39;baiduspider&#39;, &#39;Bingbot&#39;]</span>
<span class="sd">...                urls=[&#39;/&#39;, &#39;/hello&#39;, &#39;/some-page.html&#39;]])</span>

<span class="sd">As a result, you get a DataFrame with a row for each combination of</span>
<span class="sd">(user-agent, URL) indicating whether or not that particular user-agent can</span>
<span class="sd">fetch the given URL.</span>

<span class="sd">Some reasons why you might want to do that:</span>

<span class="sd">* SEO Audits: Especially for large websites with many URL patterns, and many</span>
<span class="sd">  rules for different user-agents.</span>
<span class="sd">* Developer or site owner about to make large changes</span>
<span class="sd">* Interest in strategies of certain companies</span>

<span class="sd">User-agents</span>
<span class="sd">-----------</span>

<span class="sd">In reality there are only two groups of user-agents that you need to worry</span>
<span class="sd">about:</span>

<span class="sd">* User-agents listed in the robots.txt file: For each one of those you need to</span>
<span class="sd">  check whether or not they are blocked from fetching a certain URL</span>
<span class="sd">  (or pattern).</span>
<span class="sd">* ``*`` all other user-agents: The ``*`` includes all other user-agents, so</span>
<span class="sd">  checking the rules that apply to it should take care of the rest.</span>

<span class="sd">robots.txt Testing Approach</span>
<span class="sd">---------------------------</span>

<span class="sd">1. Get the robots.txt file that you are interested in</span>
<span class="sd">2. Extract the user-agents from it</span>
<span class="sd">3. Specify the URLs you are interested in testing</span>
<span class="sd">4. Run the :func:`robotstxt_test` function</span>

<span class="sd">&gt;&gt;&gt; fb_robots = robotstxt_to_df(&#39;https://www.facebook.com/robots.txt&#39;)</span>
<span class="sd">&gt;&gt;&gt; fb_robots</span>
<span class="sd">      directive                                            content                        robotstxt_url                    download_date</span>
<span class="sd">0       comment  Notice: Collection of data on Facebook through...  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">1       comment  prohibited unless you have express written per...  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">2       comment  and may only be conducted for the limited purp...  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">3       comment                                        permission.  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">4       comment  See: http://www.facebook.com/apps/site_scrapin...  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">..          ...                                                ...                                  ...                              ...</span>
<span class="sd">461       Allow                         /ajax/bootloader-endpoint/  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">462       Allow  /ajax/pagelet/generic.php/PagePostsSectionPagelet  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">463       Allow                                      /safetycheck/  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">464  User-agent                                                  *  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">465    Disallow                                                  /  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">[466 rows x 4 columns]</span>

<span class="sd">Now that we have downloaded the file, we can easily extract the list of</span>
<span class="sd">user-agents that it contains.</span>

<span class="sd">&gt;&gt;&gt; fb_useragents = (fb_robots</span>
<span class="sd">...                  [fb_robots[&#39;directive&#39;]==&#39;User-agent&#39;]</span>
<span class="sd">...                  [&#39;content&#39;].drop_duplicates()</span>
<span class="sd">...                  .tolist())</span>
<span class="sd">&gt;&gt;&gt; fb_useragents</span>
<span class="sd">[&#39;Applebot&#39;,</span>
<span class="sd"> &#39;baiduspider&#39;,</span>
<span class="sd"> &#39;Bingbot&#39;,</span>
<span class="sd"> &#39;Discordbot&#39;,</span>
<span class="sd"> &#39;facebookexternalhit&#39;,</span>
<span class="sd"> &#39;Googlebot&#39;,</span>
<span class="sd"> &#39;Googlebot-Image&#39;,</span>
<span class="sd"> &#39;ia_archiver&#39;,</span>
<span class="sd"> &#39;LinkedInBot&#39;,</span>
<span class="sd"> &#39;msnbot&#39;,</span>
<span class="sd"> &#39;Naverbot&#39;,</span>
<span class="sd"> &#39;Pinterestbot&#39;,</span>
<span class="sd"> &#39;seznambot&#39;,</span>
<span class="sd"> &#39;Slurp&#39;,</span>
<span class="sd"> &#39;teoma&#39;,</span>
<span class="sd"> &#39;TelegramBot&#39;,</span>
<span class="sd"> &#39;Twitterbot&#39;,</span>
<span class="sd"> &#39;Yandex&#39;,</span>
<span class="sd"> &#39;Yeti&#39;,</span>
<span class="sd"> &#39;*&#39;]</span>


<span class="sd">Quite a long list!</span>

<span class="sd">As a small and quick test, I&#39;m interested in checking the home page, a random</span>
<span class="sd">profile page (/bbc), groups and hashtags pages.</span>

<span class="sd">&gt;&gt;&gt; urls_to_test = [&#39;/&#39;, &#39;/bbc&#39;, &#39;/groups&#39;, &#39;/hashtag/&#39;]</span>
<span class="sd">&gt;&gt;&gt; fb_test = robotstxt_test(&#39;https://www.facebook.com/robots.txt&#39;,</span>
<span class="sd">...                          fb_useragents, urls_to_test)</span>
<span class="sd">&gt;&gt;&gt; fb_test</span>
<span class="sd">                          robotstxt_url user_agent   url_path  can_fetch</span>
<span class="sd">0   https://www.facebook.com/robots.txt          *       /bbc      False</span>
<span class="sd">1   https://www.facebook.com/robots.txt          *    /groups      False</span>
<span class="sd">2   https://www.facebook.com/robots.txt          *          /      False</span>
<span class="sd">3   https://www.facebook.com/robots.txt          *  /hashtag/      False</span>
<span class="sd">4   https://www.facebook.com/robots.txt   Applebot          /       True</span>
<span class="sd">..                                  ...        ...        ...        ...</span>
<span class="sd">75  https://www.facebook.com/robots.txt  seznambot    /groups       True</span>
<span class="sd">76  https://www.facebook.com/robots.txt      teoma          /       True</span>
<span class="sd">77  https://www.facebook.com/robots.txt      teoma  /hashtag/      False</span>
<span class="sd">78  https://www.facebook.com/robots.txt      teoma       /bbc       True</span>
<span class="sd">79  https://www.facebook.com/robots.txt      teoma    /groups       True</span>
<span class="sd">[80 rows x 4 columns]</span>


<span class="sd">For twenty user-agents and four URLs each, we received a total of eighty test</span>
<span class="sd">results. You can immediately see that all user-agents not listed (denoted by</span>
<span class="sd">`*` are not allowed to fetch any of the provided URLs).</span>

<span class="sd">Let&#39;s see who is and who is not allowed to fetch the home page.</span>

<span class="sd">&gt;&gt;&gt; fb_test.query(&#39;url_path== &quot;/&quot;&#39;)</span>
<span class="sd">                          robotstxt_url           user_agent  url_path  can_fetch</span>
<span class="sd">2   https://www.facebook.com/robots.txt                    *         /      False</span>
<span class="sd">4   https://www.facebook.com/robots.txt             Applebot         /       True</span>
<span class="sd">9   https://www.facebook.com/robots.txt              Bingbot         /       True</span>
<span class="sd">14  https://www.facebook.com/robots.txt           Discordbot         /      False</span>
<span class="sd">18  https://www.facebook.com/robots.txt            Googlebot         /       True</span>
<span class="sd">21  https://www.facebook.com/robots.txt      Googlebot-Image         /       True</span>
<span class="sd">26  https://www.facebook.com/robots.txt          LinkedInBot         /      False</span>
<span class="sd">30  https://www.facebook.com/robots.txt             Naverbot         /       True</span>
<span class="sd">35  https://www.facebook.com/robots.txt         Pinterestbot         /      False</span>
<span class="sd">39  https://www.facebook.com/robots.txt                Slurp         /       True</span>
<span class="sd">43  https://www.facebook.com/robots.txt          TelegramBot         /      False</span>
<span class="sd">47  https://www.facebook.com/robots.txt           Twitterbot         /       True</span>
<span class="sd">48  https://www.facebook.com/robots.txt               Yandex         /       True</span>
<span class="sd">55  https://www.facebook.com/robots.txt                 Yeti         /       True</span>
<span class="sd">57  https://www.facebook.com/robots.txt          baiduspider         /       True</span>
<span class="sd">60  https://www.facebook.com/robots.txt  facebookexternalhit         /      False</span>
<span class="sd">64  https://www.facebook.com/robots.txt          ia_archiver         /      False</span>
<span class="sd">68  https://www.facebook.com/robots.txt               msnbot         /       True</span>
<span class="sd">74  https://www.facebook.com/robots.txt            seznambot         /       True</span>
<span class="sd">76  https://www.facebook.com/robots.txt                teoma         /       True</span>

<span class="sd">I&#39;ll leave it to you to figure out why LinkedIn and Pinterest are not allowed</span>
<span class="sd">to crawl the home page but Google and Apple are, because I have no clue!</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;robotstxt_to_df&#39;</span><span class="p">,</span> <span class="s1">&#39;robotstxt_test&#39;</span><span class="p">]</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">concurrent</span> <span class="kn">import</span> <span class="n">futures</span>
<span class="kn">from</span> <span class="nn">urllib.request</span> <span class="kn">import</span> <span class="n">Request</span><span class="p">,</span> <span class="n">urlopen</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>

<span class="kn">from</span> <span class="nn">protego</span> <span class="kn">import</span> <span class="n">Protego</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">from</span> <span class="nn">advertools</span> <span class="kn">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">version</span>

<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;advertools-&#39;</span> <span class="o">+</span> <span class="n">version</span><span class="p">}</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>


<div class="viewcode-block" id="robotstxt_to_df"><a class="viewcode-back" href="../../advertools.robotstxt.html#advertools.robotstxt.robotstxt_to_df">[docs]</a><span class="k">def</span> <span class="nf">robotstxt_to_df</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Download the contents of ``robotstxt_url`` into a DataFrame</span>

<span class="sd">    You can also use it to download multiple robots files by passing a list of</span>
<span class="sd">    URLs.</span>

<span class="sd">    &gt;&gt;&gt; robotstxt_to_df(&#39;https://www.twitter.com/robots.txt&#39;)</span>
<span class="sd">         directive content   	                 robotstxt_url	                   download_date</span>
<span class="sd">    0	User-agent	     *	https://www.twitter.com/robots.txt	2020-09-27 21:57:23.702814+00:00</span>
<span class="sd">    1	  Disallow	     /	https://www.twitter.com/robots.txt	2020-09-27 21:57:23.702814+00:00</span>

<span class="sd">    &gt;&gt;&gt; robotstxt_to_df([&#39;https://www.google.com/robots.txt&#39;,</span>
<span class="sd">    ...                  &#39;https://www.twitter.com/robots.txt&#39;])</span>
<span class="sd">           directive	                             content	    robotstxt_last_modified	                       robotstxt_url	                     download_date</span>
<span class="sd">    0	  User-agent	                                   *	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    1	    Disallow	                             /search	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    2	       Allow	                       /search/about	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    3	       Allow	                      /search/static	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    4	       Allow	              /search/howsearchworks	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    283	  User-agent	                 facebookexternalhit	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    284	       Allow	                             /imgres	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    285	     Sitemap	  https://www.google.com/sitemap.xml	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    286	  User-agent	                                   *	                        NaT	  https://www.twitter.com/robots.txt	  2021-01-16 14:08:50.468588+00:00</span>
<span class="sd">    287	    Disallow	                                   /	                        NaT	  https://www.twitter.com/robots.txt	  2021-01-16 14:08:50.468588+00:00</span>

<span class="sd">    For research purposes and if you want to download more than ~500 files, you</span>
<span class="sd">    might want to use ``output_file`` to save results as they are downloaded.</span>
<span class="sd">    The file extension should be &quot;.jl&quot;, and robots files are appended to that</span>
<span class="sd">    file as soon as they are downloaded, in case you lose your connection, or</span>
<span class="sd">    maybe your patience!</span>

<span class="sd">    &gt;&gt;&gt; robotstxt_to_df([&#39;https://example.com/robots.txt&#39;,</span>
<span class="sd">    ...                  &#39;https://example.com/robots.txt&#39;,</span>
<span class="sd">    ...                  &#39;https://example.com/robots.txt&#39;],</span>
<span class="sd">    ...                 output_file=&#39;robots_output_file.jl&#39;)</span>

<span class="sd">    To open the file as a DataFrame:</span>

<span class="sd">    &gt;&gt;&gt; import pandas as pd</span>
<span class="sd">    &gt;&gt;&gt; robotsfiles_df = pd.read_json(&#39;robots_output_file.jl&#39;, lines=True)</span>

<span class="sd">    :param url robotstxt_url: One or more URLs of the robots.txt file(s)</span>
<span class="sd">    :param str output_file: Optional file path to save the robots.txt files,</span>
<span class="sd">                            mainly useful for downloading &gt; 500 files. The</span>
<span class="sd">                            files are appended as soon as they are downloaded.</span>
<span class="sd">                            Only &quot;.jl&quot; extensions are supported.</span>

<span class="sd">    :returns DataFrame robotstxt_df: A DataFrame containing directives, their</span>
<span class="sd">                                     content, the URL and time of download</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">output_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">output_file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.jl&#39;</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Please specify a file with a `.jl` extension.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">set</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">_robots_multi</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="n">output_file</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">msg</span><span class="o">=</span><span class="s1">&#39;Getting: &#39;</span> <span class="o">+</span> <span class="n">robotstxt_url</span><span class="p">)</span>
            <span class="n">robots_open</span> <span class="o">=</span> <span class="n">urlopen</span><span class="p">(</span><span class="n">Request</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">))</span>
            <span class="n">robots_text</span> <span class="o">=</span> <span class="n">robots_open</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
            <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">robots_text</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8-sig&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;#&#39;</span><span class="p">):</span>
                        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="s1">&#39;comment&#39;</span><span class="p">,</span>
                                      <span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8-sig&#39;</span><span class="p">)</span>
                                           <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;#&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">())])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">split</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8-sig&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">maxsplit</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">split</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">split</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()])</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;directive&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">])</span>
            <span class="n">etag_lastmod</span> <span class="o">=</span> <span class="p">{</span><span class="n">header</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">):</span> <span class="n">val</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;&quot;&#39;</span><span class="p">)</span>
                            <span class="k">for</span> <span class="n">header</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">robots_open</span><span class="o">.</span><span class="n">getheaders</span><span class="p">()</span>
                            <span class="k">if</span> <span class="n">header</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;etag&#39;</span><span class="p">,</span> <span class="s1">&#39;last-modified&#39;</span><span class="p">]}</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="o">**</span><span class="n">etag_lastmod</span><span class="p">)</span>
            <span class="k">if</span> <span class="s1">&#39;last_modified&#39;</span> <span class="ow">in</span> <span class="n">df</span><span class="p">:</span>
                <span class="n">df</span><span class="p">[</span><span class="s1">&#39;robotstxt_last_modified&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;last_modified&#39;</span><span class="p">])</span>
                <span class="k">del</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;last_modified&#39;</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;errors&#39;</span><span class="p">:</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)]})</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;robotstxt_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">robotstxt_url</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;download_date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Timestamp</span><span class="o">.</span><span class="n">now</span><span class="p">(</span><span class="n">tz</span><span class="o">=</span><span class="s1">&#39;UTC&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">output_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
                <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">to_json</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s1">&#39;records&#39;</span><span class="p">,</span>
                                      <span class="n">lines</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">date_format</span><span class="o">=</span><span class="s1">&#39;iso&#39;</span><span class="p">))</span>
                <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">df</span></div>


<span class="k">def</span> <span class="nf">_robots_multi</span><span class="p">(</span><span class="n">robots_url_list</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">final_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">futures</span><span class="o">.</span><span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
        <span class="n">to_do</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">robotsurl</span> <span class="ow">in</span> <span class="n">robots_url_list</span><span class="p">:</span>
            <span class="n">future</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">robotstxt_to_df</span><span class="p">,</span> <span class="n">robotsurl</span><span class="p">)</span>
            <span class="n">to_do</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>
        <span class="n">done_iter</span> <span class="o">=</span> <span class="n">futures</span><span class="o">.</span><span class="n">as_completed</span><span class="p">(</span><span class="n">to_do</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">future</span> <span class="ow">in</span> <span class="n">done_iter</span><span class="p">:</span>
            <span class="n">future_result</span> <span class="o">=</span> <span class="n">future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">output_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
                    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">future_result</span><span class="o">.</span><span class="n">to_json</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s1">&#39;records&#39;</span><span class="p">,</span>
                                                     <span class="n">lines</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                     <span class="n">date_format</span><span class="o">=</span><span class="s1">&#39;iso&#39;</span><span class="p">))</span>
                    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">final_df</span> <span class="o">=</span> <span class="n">final_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">future_result</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_file</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">final_df</span>


<div class="viewcode-block" id="robotstxt_test"><a class="viewcode-back" href="../../advertools.robotstxt.html#advertools.robotstxt.robotstxt_test">[docs]</a><span class="k">def</span> <span class="nf">robotstxt_test</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="n">user_agents</span><span class="p">,</span> <span class="n">urls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Given a :attr:`robotstxt_url` check which of the :attr:`user_agents` is</span>
<span class="sd">    allowed to fetch which of the :attr:`urls`.</span>

<span class="sd">    All the combinations of :attr:`user_agents` and :attr:`urls` will be</span>
<span class="sd">    checked and the results returned in one DataFrame.</span>

<span class="sd">    &gt;&gt;&gt; robotstxt_test(&#39;https://facebook.com/robots.txt&#39;,</span>
<span class="sd">    ...                user_agents=[&#39;*&#39;, &#39;Googlebot&#39;, &#39;Applebot&#39;],</span>
<span class="sd">    ...                urls=[&#39;/&#39;, &#39;/bbc&#39;, &#39;/groups&#39;, &#39;/hashtag/&#39;])</span>
<span class="sd">                          robotstxt_url user_agent   url_path  can_fetch</span>
<span class="sd">    0   https://facebook.com/robots.txt          *          /      False</span>
<span class="sd">    1   https://facebook.com/robots.txt          *       /bbc      False</span>
<span class="sd">    2   https://facebook.com/robots.txt          *    /groups      False</span>
<span class="sd">    3   https://facebook.com/robots.txt          *  /hashtag/      False</span>
<span class="sd">    4   https://facebook.com/robots.txt   Applebot          /       True</span>
<span class="sd">    5   https://facebook.com/robots.txt   Applebot       /bbc       True</span>
<span class="sd">    6   https://facebook.com/robots.txt   Applebot    /groups       True</span>
<span class="sd">    7   https://facebook.com/robots.txt   Applebot  /hashtag/      False</span>
<span class="sd">    8   https://facebook.com/robots.txt  Googlebot          /       True</span>
<span class="sd">    9   https://facebook.com/robots.txt  Googlebot       /bbc       True</span>
<span class="sd">    10  https://facebook.com/robots.txt  Googlebot    /groups       True</span>
<span class="sd">    11  https://facebook.com/robots.txt  Googlebot  /hashtag/      False</span>

<span class="sd">    :param url robotstxt_url: The URL of robotx.txt file</span>
<span class="sd">    :param str,list user_agents: One or more user agents</span>
<span class="sd">    :param str,list urls: One or more paths (relative) or URLs (absolute) to</span>
<span class="sd">                           check</span>
<span class="sd">    :return DataFrame robotstxt_test_df:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">robotstxt_url</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;/robots.txt&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Please make sure you enter a valid robots.txt URL&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">user_agents</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">user_agents</span> <span class="o">=</span> <span class="p">[</span><span class="n">user_agents</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">urls</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span><span class="n">urls</span><span class="p">]</span>
    <span class="n">robots_open</span> <span class="o">=</span> <span class="n">urlopen</span><span class="p">(</span><span class="n">Request</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">))</span>
    <span class="n">robots_bytes</span> <span class="o">=</span> <span class="n">robots_open</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="n">robots_text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">robots_bytes</span><span class="p">)</span>
    <span class="n">rp</span> <span class="o">=</span> <span class="n">Protego</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">robots_text</span><span class="p">)</span>

    <span class="n">test_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="n">urls</span><span class="p">,</span> <span class="n">user_agents</span><span class="p">):</span>
        <span class="n">d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;user_agent&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;url_path&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">path</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;can_fetch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rp</span><span class="o">.</span><span class="n">can_fetch</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
        <span class="n">test_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_list</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;robotstxt_url&#39;</span><span class="p">,</span> <span class="n">robotstxt_url</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">([</span><span class="s1">&#39;user_agent&#39;</span><span class="p">,</span> <span class="s1">&#39;url_path&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Elias Dabbas.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>